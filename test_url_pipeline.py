
import requests
import os
from supabase import create_client
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import datetime

load_dotenv()

supabase_url = os.environ.get("SUPABASE_URL") or os.environ.get("VITE_SUPABASE_URL")
supabase_key = os.environ.get("SUPABASE_ANON_KEY") or os.environ.get("VITE_SUPABASE_ANON_KEY")

if not supabase_url or not supabase_key:
    print("‚ùå Error: Supabase credentials missing.")
    exit(1)

sb = create_client(supabase_url, supabase_key)

TARGET_URL = "http://spot.rassiro.com/rd/20251211/1000323"
ORIGINAL_MSG = "[rassiro_channel] [Î¶¨Ìè¨Ìä∏ Î∏åÎ¶¨Ìïë]ÏóêÏä§Ïó†Ïî®ÏßÄ, 'Ïú†Î¶¨Ïö©Í∏∞Îäî ÏãúÍ∞ÑÏùÑ Îì§Ïó¨Ïïº...' Not Rated - ÌÇ§ÏõÄÏ¶ùÍ∂å"

def fetch_url_content(url):
    try:
        print(f"üåç Fetching URL: {url}...")
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Try to find main content (heuristics for rassiro/news sites)
        text = ""
        article = soup.find('div', class_='news_body') or soup.find('div', class_='article_view') or soup.find('body')
        if article:
            text = article.get_text(separator=' ', strip=True)
        else:
            text = soup.get_text(separator=' ', strip=True)
            
        return text[:1000] # Limit to 1000 chars
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to fetch URL: {e}")
        return "(Content fetch failed - using simulation)"

def simulate_pipeline():
    # 1. Fetch
    content = fetch_url_content(TARGET_URL)
    
    if "Content fetch failed" in content:
        # Fallback simulation if URL is dead/unreachable
        content = "ÏóêÏä§Ïó†Ïî®ÏßÄ(SMCG)Ïóê ÎåÄÌï¥ ÌÇ§ÏõÄÏ¶ùÍ∂åÏùÄ Ïú†Î¶¨Ïö©Í∏∞ ÏÇ∞ÏóÖÏùò ÌäπÏÑ±ÏÉÅ ÏãúÍ∞ÑÏùÑ Îì§Ïó¨Ïïº Í∞ÄÏπòÍ∞Ä ÎìúÎü¨ÎÇúÎã§Í≥† ÌèâÍ∞ÄÌñàÎã§. Ìà¨ÏûêÏùòÍ≤¨ÏùÄ Not Rated, Î™©ÌëúÍ∞ÄÎäî Ï†úÏãúÌïòÏßÄ ÏïäÏïòÎã§. ÎèôÏÇ¨Îäî ÌôîÏû•Ìíà Ïú†Î¶¨Ïö©Í∏∞ Ï†úÏ°∞ÏÇ¨Î°ú..."

    print(f"üìù Extracted Content: {content[:50]}...")

    # 2. Construct Enhanced Message (Simulating what a Crawler would do)
    full_message = f"{ORIGINAL_MSG}\n\n[Auto-Crawled Summary]:\n{content}\n\nOriginal Link: {TARGET_URL}"

    # 3. Insert into Supabase
    print("üöÄ Inserting into 'telegram_messages'...")
    data = {
        "channel": "rassiro_channel",
        "message": full_message,
        "created_at": datetime.datetime.utcnow().isoformat()
    }
    
    try:
        res = sb.table("telegram_messages").insert(data).execute()
        print("‚úÖ Message Inserted Successfully.")
        print("üí° The Dashboard should now pick this up, and since it is long (>100 chars), the AI will analyze it.")
    except Exception as e:
        print(f"‚ùå Insert Failed: {e}")

if __name__ == "__main__":
    simulate_pipeline()
